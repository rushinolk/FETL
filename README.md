# Pipeline de ETL de Dados de Produtos (Fake Store API)

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![Libraries](https://img.shields.io/badge/Libraries-Pandas%20%7C%20Requests-orange.svg)
![Cloud](https://img.shields.io/badge/Cloud-Google%20BigQuery-red.svg)

## ğŸ“– DescriÃ§Ã£o

Este projeto implementa um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) completo e automatizado. O pipeline orquestra a extraÃ§Ã£o de dados de produtos de uma API REST pÃºblica (Fake Store API), processa e limpa esses dados utilizando a biblioteca Pandas, e consolida o resultado em uma tabela no Google BigQuery, pronta para anÃ¡lises de Business Intelligence.

---

## âœ¨ Principais Funcionalidades

* **ExtraÃ§Ã£o de API:** ConexÃ£o com uma fonte de dados externa via REST API, com tratamento de erros de rede.
* **TransformaÃ§Ã£o de Dados com Pandas:** Limpeza, seleÃ§Ã£o de colunas, e achatamento (flattening) de estruturas JSON aninhadas para um formato tabular.
* **Engenharia de Features:** CriaÃ§Ã£o de novas colunas para enriquecer o dataset (ex: `data_hora_carga`).
* **IntegraÃ§Ã£o com Cloud Data Warehouse:** Carga dos dados transformados diretamente no Google BigQuery, demonstrando a integraÃ§Ã£o com serviÃ§os de nuvem.
* **Gerenciamento Seguro de Credenciais:** UtilizaÃ§Ã£o de variÃ¡veis de ambiente para gerenciar as chaves de acesso ao GCP, evitando a exposiÃ§Ã£o de informaÃ§Ãµes sensÃ­veis no cÃ³digo.

---

## ğŸ—ï¸ Arquitetura do Pipeline

O fluxo de dados segue o padrÃ£o ETL clÃ¡ssico:

```mermaid
graph TD
    A[Fonte: Fake Store API] -->|Passo 1: ExtraÃ§Ã£o| B[Script Python em .py];
    B -->|Passo 2: TransformaÃ§Ã£o com Pandas| C{DataFrame Limpo e Estruturado};
    C -->|Passo 3: Carga via pandas-gbq| D[(Destino: Google BigQuery)];
```

---

## ğŸš€ Tecnologias Utilizadas

* **Linguagem:** Python 3.9+
* **Bibliotecas Principais:** Pandas, Requests, Google Cloud BigQuery, Pandas-GBQ
* **Cloud:** Google BigQuery
* **Ambiente e DependÃªncias:** `venv`, `pip`

---

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ credentials/
â”‚   â””â”€â”€ gcp_service_account.json  # (Adicionar manualmente)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_pipeline.py           # MÃ³dulo com a lÃ³gica de E, T, e L
â”‚   â””â”€â”€ main.py                   # Ponto de entrada do projeto
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config.py                     # Arquivo de configuraÃ§Ã£o
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Como Executar o Projeto

Siga os passos abaixo para executar o pipeline no seu ambiente local.

### PrÃ©-requisitos
* Python 3.9 ou superior instalado.
* Uma conta no Google Cloud Platform com um projeto criado e a API do BigQuery ativada.
* O arquivo de credencial (`.json`) de uma Service Account do GCP com permissÃµes de "UsuÃ¡rio do BigQuery" e "Editor de Dados do BigQuery".

### Passos

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
    cd seu-repositorio
    ```

2.  **Crie e ative o ambiente virtual:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # No Windows: venv\Scripts\activate
    ```

3.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure as credenciais:**
    * Coloque seu arquivo de credencial `.json` do GCP dentro da pasta `credentials/`.
    * **Importante:** Renomeie o arquivo para `gcp_service_account.json` para que corresponda ao caminho no cÃ³digo.

5.  **Configure as variÃ¡veis do projeto:**
    * Abra o arquivo `config.py` e preencha as variÃ¡veis `GCP_PROJECT_ID`, `BIGQUERY_DATASET` e `BIGQUERY_TABLE_NAME` com as informaÃ§Ãµes do seu projeto no Google Cloud.

6.  **Execute o pipeline:**
    * A partir da pasta **raiz** do projeto, execute o comando:
    ```bash
    python -m src.main
    ```

---

## âœ… Resultado Esperado

ApÃ³s a execuÃ§Ã£o, vocÃª verÃ¡ no terminal as mensagens de log indicando o sucesso de cada etapa (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga).

Ao final, vocÃª poderÃ¡ acessar seu projeto no console do Google BigQuery e encontrar a tabela `sales` (ou o nome que vocÃª definiu) dentro do seu dataset, populada com os dados de produtos jÃ¡ limpos e estruturados.
